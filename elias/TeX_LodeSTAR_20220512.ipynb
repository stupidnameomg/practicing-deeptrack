{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 256,
      "metadata": {
        "collapsed": true,
        "id": "ZXRzLVLHMGa1"
      },
      "outputs": [],
      "source": [
        "\"\"\"Import everything we need\"\"\"\n",
        "already_installed = !pip list -v | grep deeptrack\n",
        "if not already_installed:\n",
        "    !pip install deeptrack==1.0.1\n",
        "    !pip install mat73\n",
        "    !pip install tqdm\n",
        "    !apt install texlive-fonts-recommended texlive-fonts-extra cm-super dvipng\n",
        "    !apt install texlive-science\n",
        "\n",
        "import matplotlib\n",
        "import os\n",
        "import random\n",
        "import mat73\n",
        "import scipy.io\n",
        "import seaborn\n",
        "import locale\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import deeptrack as dt\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.math import abs, angle, real, imag, subtract, multiply\n",
        "from scipy.special import binom\n",
        "from matplotlib import rc\n",
        "from scipy.stats import chi2, betaprime, norm\n",
        "from scipy.optimize import minimize\n",
        "from matplotlib import colors\n",
        "from cycler import cycler\n",
        "from google.colab import drive\n",
        "from tqdm import tqdm\n",
        "from math import factorial\n",
        "\n",
        "\n",
        "import matplotlib.ticker as tkr\n",
        "\n",
        "def func(x, pos):  # formatter function takes tick label and tick position\n",
        "    s = str(x)\n",
        "    ind = s.index('.')\n",
        "    return s[:ind] + ',' + s[ind+1:]   # change dot to comma\n",
        "\n",
        "y_format = tkr.FuncFormatter(func)\n",
        "rc('text', usetex=True)\n",
        "matplotlib.rcParams['text.latex.preamble'] = [r'\\usepackage{amsmath}', r'\\usepackage{siunitx}', r'\\usepackage{lipsum}']\n",
        "plt.rcParams['axes.formatter.use_locale'] = True\n",
        "plt.rcParams['font.size'] = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 257,
      "metadata": {
        "cellView": "form",
        "id": "_KSBuroMllya"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "\"\"\"Import the validation set\"\"\"\n",
        "\"\"\" Function to import used to unpack rows of FFT-field\"\"\"\n",
        "def exp_expand(data,shape=[64,64]):\n",
        "    x = np.arange(shape[0]) - shape[0] / 2\n",
        "    y = np.arange(shape[1]) - shape[1] / 2\n",
        "    X, Y = np.meshgrid(x, y)\n",
        "    bg=np.zeros((shape[0]*shape[1],))+0j\n",
        "    X=np.reshape(np.fft.fftshift(X),(shape[0]*shape[1]))\n",
        "    Y=np.reshape(np.fft.fftshift(Y),(shape[0]*shape[1]))\n",
        "    RHO=X**2+Y**2\n",
        "    Inds=np.argsort(RHO)\n",
        "    Inds=np.sort(Inds[:data.shape[0]])\n",
        "\n",
        "    bg[Inds[:data.shape[0]]]=data\n",
        "    bg=np.reshape(bg,(shape[0],shape[1]))\n",
        "  \n",
        "    return bg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 258,
      "metadata": {
        "id": "zGUDvGJRQd0D"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "def is_arithmetic(l):\n",
        "    for index in range(len(l) - 1):\n",
        "        if not (l[index + 1] - l[index] == 1):\n",
        "            is_arithmetic.counter += 1\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def extract_pos_and_images(data, shortest_trace=20):\n",
        "    \"\"\"Saves images and positions for individual particles.\"\"\"\n",
        "    is_arithmetic.counter = 0\n",
        "    particles_timeseries = []\n",
        "    particles_positions = []\n",
        "    num_particles = data.shape[1]\n",
        "\n",
        "    for i in tqdm(range(num_particles)):\n",
        "        roi_data = np.array(data[0,i][0,0][\"ROI\"])\n",
        "        if shortest_trace <= roi_data[0,0].shape[0] and is_arithmetic(data[0,i][0,0][\"Positions\"][:,0].flatten()):\n",
        "            positions = data[0,i][0,0][\"Positions\"][:,1:4]\n",
        "            particles_positions.append(positions)\n",
        "\n",
        "            timeseries = []\n",
        "\n",
        "            for j in range(roi_data[0,0].shape[0]):\n",
        "                particleROI = np.expand_dims(np.fft.fftshift(np.fft.ifft2(exp_expand(roi_data[0,0][j,:]))), axis = -1)\n",
        "                timeseries.append(particleROI)\n",
        "            particles_timeseries.append(np.array(timeseries))\n",
        "    print(f\"Number of traces discarded because of non-constant time-steps: {is_arithmetic.counter}\")\n",
        "    return particles_positions, particles_timeseries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 259,
      "metadata": {
        "id": "T6Pvy7aWMGa9"
      },
      "outputs": [],
      "source": [
        "def create_group(crop_size=64, padding=16, wavelength=525e-9, pixel_size=114e-9):\n",
        "    crop_propagation_forward = dt.get_propagation_matrix((crop_size + 2 * padding, ) * 2, 1e-7, pixel_size, wavelength)\n",
        "    crop_propagation_backward = dt.get_propagation_matrix((crop_size + 2 * padding, ) * 2, -1e-7, pixel_size, wavelength)\n",
        "\n",
        "    args = dt.Arguments(\n",
        "        translate=lambda: np.random.randn(2)*0.5,\n",
        "        rotate=lambda: np.random.uniform(0, np.pi*2),\n",
        "        z=lambda: np.random.randn(1)*20,\n",
        "    )\n",
        "\n",
        "    group = ( \n",
        "        dt.Affine(rotate=args.rotate) \n",
        "        >> dt.Affine(translate=args.translate) \n",
        "        >> dt.FourierTransform(padding=padding) \n",
        "        >> dt.FourierTransformTransformation(crop_propagation_forward, crop_propagation_backward, args.z, padding=padding) \n",
        "        >> dt.InverseFourierTransform(padding=padding) \n",
        "    )\n",
        "\n",
        "    equivariance = (\n",
        "        dt.models.lodestar.Rotational2DEquivariance(args.rotate)\n",
        "        >> dt.models.lodestar.TranslationalEquivariance(args.translate)\n",
        "        >> dt.models.lodestar.Equivariance(1, args.z/10, indexes=2)\n",
        "    )\n",
        "\n",
        "    return group, equivariance, (crop_propagation_forward, crop_propagation_backward)\n",
        "\n",
        "def create_training_set(particles_timeseries):\n",
        "    imgs = []\n",
        "    for particle_id in range(0,len(particles_timeseries),1):\n",
        "        img = np.stack((np.real(particles_timeseries[particle_id])[:,:,:,0], np.imag(particles_timeseries[particle_id][:,:,:,0])), axis=-1)\n",
        "        img = img[0,:,:,:]\n",
        "        imgs.append(img)\n",
        "    return dt.Value(lambda: random.choice(np.array(imgs)))\n",
        "\n",
        "def plot_dataset(dataset, group, nrows=3, ncols=10):\n",
        "    for _ in range(nrows):\n",
        "        fig, axs = plt.subplots(1,ncols,figsize=(25,8))\n",
        "        for i in range(ncols):\n",
        "          axs[i].imshow((dataset>>group).update()()[..., 1])\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 260,
      "metadata": {
        "id": "rq0XzGTFjmnQ"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "\"\"\"Create custom layers\"\"\"\n",
        "class CustomLayer(keras.layers.Layer):\n",
        "    \"Takes 2 layers as input and outputs 4 layers.\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def build( self, input_shape):\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, image):\n",
        "        image = tf.dtypes.complex(image[...,0], image[...,1])\n",
        "        norm_image = subtract(image, 1)\n",
        "        return tf.stack([abs(norm_image), angle(image), real(norm_image), imag(norm_image)], axis=-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 261,
      "metadata": {
        "collapsed": true,
        "id": "g3v8o4Nk4ZkD"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "\"\"\"Create models\"\"\"\n",
        "def create_model(custom_layer, num_layers, image_size, num_outputs):\n",
        "    model = keras.models.Sequential()\n",
        "    model.add(keras.Input(shape=(image_size,image_size,2)))\n",
        "    model.add(custom_layer)\n",
        "    model.add(keras.layers.Conv2D(64, 3, activation='relu', strides=(2,2)))\n",
        "    for _ in range(num_layers):\n",
        "        model.add(keras.layers.Conv2D(64, 3, activation='relu'))\n",
        "    model.add(keras.layers.Conv2D(num_outputs+1,1))\n",
        "    \n",
        "    model = dt.models.LodeSTAR(model=model, input_shape=(None, None, 2), num_outputs=num_outputs)\n",
        "    model.specs = {\"Layers\": num_layers}\n",
        "    model.totalhistory = {\"total_loss\":[], \"consistency_loss\":[]}\n",
        "    model.compile(loss=[\"mae\", \"mae\", \"mae\"], optimizer=tf.keras.optimizers.Adam(), loss_weights=[1, 1, 1])\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 262,
      "metadata": {
        "id": "a5cBmiGxwnin"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "def save_progress(model, h):\n",
        "    model.totalhistory[\"total_loss\"].extend(h.history[\"total_loss\"])\n",
        "    model.totalhistory[\"consistency_loss\"].extend(h.history[\"consistency_loss\"])\n",
        "\n",
        "def plot_progress(models):\n",
        "    def moving_average(x, w):\n",
        "        return np.convolve(x, np.ones(w), 'valid') / w\n",
        "\n",
        "    cc = cycler(color=list('bgrcmyk'))\n",
        "    fig, (ax1, ax2) = plt.subplots(1,2,figsize=(15,6))\n",
        "    ax1.set_prop_cycle(cc)\n",
        "    ax2.set_prop_cycle(cc)\n",
        "\n",
        "    for i, model in enumerate(models):\n",
        "        ax1.plot(moving_average(model.totalhistory[\"total_loss\"], 10), label=f\"{model.specs}\")\n",
        "        ax1.set_xlabel(\"Epochs\")\n",
        "        ax1.set_ylabel(\"Total Loss\")\n",
        "        ax1.set_yscale(\"log\")\n",
        "        ax1.legend(prop={'size': 15})\n",
        "\n",
        "        ax2.plot(moving_average(model.totalhistory[\"consistency_loss\"], 10), label=f\"{model.specs}\")\n",
        "        ax2.set_xlabel(\"Epochs\")\n",
        "        ax2.set_ylabel(\"Consistency loss\")\n",
        "        ax2.set_yscale(\"log\")\n",
        "        ax2.legend(prop={'size': 15})\n",
        "\n",
        "    ax1.grid(True)\n",
        "    ax2.grid(True)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 263,
      "metadata": {
        "cellView": "form",
        "id": "gCedQikgV1qE"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "def get_propagation_matrix(shape, to_z, pixel_size, wavelength):\n",
        "\n",
        "    k = 2 * np.pi / wavelength\n",
        "    yr, xr, *_ = shape\n",
        "\n",
        "    x = 2 * np.pi / pixel_size * np.arange(-(xr / 2 - 1 / 2), (xr / 2 + 1 / 2), 1) / xr\n",
        "    y = 2 * np.pi / pixel_size * np.arange(-(yr / 2 - 1 / 2), (yr / 2 + 1 / 2), 1) / yr\n",
        "    KXk, KYk = np.meshgrid(x, y)\n",
        "\n",
        "    K = np.real(\n",
        "        np.sqrt(np.array(1 - (KXk / k) ** 2 - (KYk / k) ** 2, dtype=np.complex64))\n",
        "    )\n",
        "    C = np.fft.fftshift(((KXk / k) ** 2 + (KYk / k) ** 2 < 1) * 1.0)\n",
        "\n",
        "    return C * np.fft.fftshift(np.exp(k * 1j * to_z * (K - 1)))\n",
        "\n",
        "def fourier_transform(image, padding=32):\n",
        "    im = np.pad(image, ((padding, padding), (padding, padding)), mode=\"symmetric\")\n",
        "    return np.fft.fft2(im)\n",
        "\n",
        "def inverse_fourier_transform(image, padding=32):\n",
        "    im = np.fft.ifft2(image)\n",
        "    return im[padding:-padding, padding:-padding]\n",
        "\n",
        "def fourier_transform_transformation(image, Tz, Tzinv, i):\n",
        "    if i < 0:\n",
        "        image *= Tzinv ** np.abs(i)\n",
        "    else:\n",
        "        image *= Tz ** i\n",
        "    return image\n",
        "\n",
        "def propagate(image, z, prop_matrices, padding):\n",
        "    \"\"\"Re-propagate the images using PAS\"\"\"\n",
        "    crop_propagation_forward, crop_propagation_backward = prop_matrices\n",
        "    ft_image = fourier_transform(image[:,:,0], padding=padding)\n",
        "    prop_ft_image = fourier_transform_transformation(ft_image, crop_propagation_backward, crop_propagation_forward, -z)\n",
        "    return np.expand_dims(inverse_fourier_transform(prop_ft_image, padding=padding),axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 264,
      "metadata": {
        "id": "-Pi0le2CzY6q"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "def predict_on_stack(model, val_stack):\n",
        "    \"\"\"returns a numpy tensor containing propagated distance as well as the prediction, for ever propagation distance and every image images\"\"\"\n",
        "    try:\n",
        "        dets = np.array(model.predict_and_detect(val_stack))#mode=ratio\n",
        "    except ValueError as E:\n",
        "        dets = np.expand_dims(np.zeros((val_stack.shape[0],3)), axis=0)\n",
        "        print(E)\n",
        "    dets = reshape_dets(dets, val_stack.shape[2])\n",
        "    return dets\n",
        "\n",
        "def reshape_dets(dets, image_size):\n",
        "    \"\"\"Removes all particles but the one closest to the center. If no particles are detected, sets the value to 0.\"\"\"\n",
        "    out = []\n",
        "    for coords in dets:\n",
        "        if coords.shape != (0, 3):\n",
        "            x, y = image_size//2, image_size//2\n",
        "            idx = np.argmin(np.array(coords[:,:2]- np.array((x, y))), axis=0)[0]\n",
        "            out.append(coords[idx,:])\n",
        "        else:\n",
        "            print(\"DID NOT FIND AND PARTICLE IN IMAGE; ADDING [0,0,0] AS CORRECTION.\")\n",
        "            out.append([0,0,0])\n",
        "    return np.array(out)\n",
        "\n",
        "def create_validation_stack(images, z_list, prop_matrices, padding):\n",
        "    \"\"\"Creates a set of images propagated to various focal planes, given by z_list.\"\"\"\n",
        "    output = []\n",
        "    for i in range(images.shape[0]):\n",
        "        image = images[i,:,:,:]\n",
        "        stack = []\n",
        "        for z in z_list:\n",
        "            stack.append(propagate(image, z, prop_matrices, padding))\n",
        "        output.append(stack)\n",
        "    return np.stack((np.real(output), np.imag(output)), axis=-1)[:,:,:,:,0,:]\n",
        "\n",
        "def evaluate_model(model, val_stack, prop_range):\n",
        "    \"\"\"Evaluates a model given a set of repropagated images, \n",
        "    by checking if the model is internally consistent in the determination of the z-coordinate\"\"\"\n",
        "    accs=[]\n",
        "    for i in range(min(val_stack.shape[0], 15)):\n",
        "        acc = np.array([predict_on_stack(model, val_stack[i])[:,2]])\n",
        "        acc = acc - np.mean(acc)\n",
        "        accs.append(acc)\n",
        "    accs = np.array(accs)\n",
        "    \n",
        "    fig, axs = plt.subplots(1,2, figsize=(16,6))\n",
        "    \n",
        "    lines = []\n",
        "    for i in range(accs.shape[0]):\n",
        "        lines.append(axs[0].plot(prop_range*1e-1, accs[i,:], alpha=0.5))\n",
        "        axs[0].set_title(\"Numeriskt ompropagerade bilder\", fontsize=20)\n",
        "        axs[0].set_xlabel(r\"Propagerat avstånd [\\SI{}{\\micro\\meter}]\", fontsize=16)\n",
        "        axs[0].set_ylabel(r\"Predikterat avstånd [\\SI{}{\\micro\\meter}]\", fontsize=16)\n",
        "        axs[0].set_xticks(np.linspace(prop_range[0]*1e-1, prop_range[-1]*1e-1, 9))\n",
        "        axs[0].set_yticks(np.linspace(prop_range[0]*1e-1-2, prop_range[-1]*1e-1+2, 11))\n",
        "\n",
        "        #lower = np.round(min(np.min(all_predictions), -8))\n",
        "        #upper = np.ceil(max(np.max(all_predictions)+1, 9))\n",
        "        # axs[0].set_yticks(np.arange(lower, upper, 1))\n",
        "        axs[0].grid(True)\n",
        "\n",
        "\n",
        "\n",
        "    line, = axs[0].plot(prop_range*1e-1, prop_range*1e-1, '--k', alpha=1, linewidth=2)\n",
        "    ln = plt.Line2D((0,1),(0,0), color='k')\n",
        "    axs[0].legend([line, ln], [\"Korrekt prediktion\", \"Prediktion på individuella partiklar\"], fontsize=12)\n",
        "    axs[0].tick_params(labelsize=16)\n",
        "    fig.suptitle('Validering av LodeSTAR', fontsize=24)\n",
        "\n",
        "    # plt.yticks(fontsize=16)\n",
        "    # plt.xticks(fontsize=16)\n",
        "    \n",
        "    for i in range(accs.shape[-1]):\n",
        "        axs[1].scatter((prop_range)[i]*1e-1, np.std(accs[:,i]), c=\"r\", marker=\".\", alpha=0.95)\n",
        "        axs[1].set_title(\"Standardavvikelse för numeriskt ompropagerade bilder\", fontsize=20)\n",
        "        axs[1].set_ylabel(r\"Standardavvikelse i predikterat avstånd [\\SI{}{\\micro\\meter}]\", fontsize=16)\n",
        "        axs[1].set_xlabel(r\"Propagerat avstånd [\\SI{}{\\micro\\meter}]\", fontsize=16)\n",
        "        axs[1].set_xticks(np.linspace(prop_range[0]*1e-1, prop_range[-1]*1e-1, 9))\n",
        "        axs[1].set_ylim(0, 2)\n",
        "        plt.yticks(fontsize=16)\n",
        "        plt.xticks(fontsize=16)\n",
        "        axs[1].grid(True)\n",
        "    plt.savefig(\"blablabla.pdf\", pad_inches=0.05, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "def plot_consistency(model, validation_set, prop_range, prop_matrices, padding):\n",
        "    val_stack = create_validation_stack(validation_set, prop_range, prop_matrices, padding)\n",
        "    evaluate_model(model, val_stack, prop_range)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 265,
      "metadata": {
        "id": "O-M2iXSlq8GD"
      },
      "outputs": [],
      "source": [
        "def calc_sigma2(arr):\n",
        "    \"\"\"\n",
        "    Takes a time series array as input and returns a CVE of sigma^2.\n",
        "\n",
        "    Source:\n",
        "    https://sci-hub.hkvisa.net/10.1103/PhysRevE.89.022726\n",
        "    \"\"\"\n",
        "    sigma2 = -np.mean(\n",
        "        np.multiply(\n",
        "            np.diff(arr[:-1]), \n",
        "            np.diff(arr[1:])\n",
        "            ))\n",
        "    return sigma2\n",
        "\n",
        "def calculate_diffusion(traces, delta_time, shortest_trace=20):\n",
        "    \"\"\"\n",
        "    Takes a list of traces with x,y and z-positions. Returns estiamtes of the\n",
        "    diffusion constant in the x and z-directions.\n",
        "\n",
        "    Source:\n",
        "    https://sci-hub.hkvisa.net/10.1103/PhysRevE.89.022726\n",
        "    \"\"\"\n",
        "    _traces = [trace for trace in traces if trace.shape[1]>shortest_trace]\n",
        "\n",
        "    sigma_x2 = np.array([calc_sigma2(trace[0,:]) for trace in _traces])\n",
        "    sigma_z2 = np.array([calc_sigma2(trace[2,:]) for trace in _traces])\n",
        "    print(f\"Median av sigma_z^2: {np.median(sigma_z2)}\")\n",
        "\n",
        "    D_x, D_z = [], []\n",
        "    for i, pos in enumerate(_traces):\n",
        "      D_x.append((\n",
        "          np.mean(np.diff(pos[0,:])**2)\n",
        "          -2*sigma_x2[i])\n",
        "          /(2*delta_time)\n",
        "          )\n",
        "      D_z.append((\n",
        "          np.mean(np.diff(pos[2,:])**2)\n",
        "          -2*sigma_z2[i])\n",
        "          /(2*delta_time)\n",
        "          )\n",
        "    print(f\"sigma2_x, sigma2_z: {np.median(sigma_x2):.5f}, {np.median(sigma_z2):.5f}\")\n",
        "    return D_x, D_z, sigma_z2\n",
        "\n",
        "\n",
        "def confint(Dz, Dx, conf=0.99):\n",
        "    \"\"\"Takes diffusion constants Dz and Dx as numpy arrays.\n",
        "    Returns confidence interval for median(Dz)/median(Dx) where significance is defined by conf.\n",
        "\n",
        "    Source:\n",
        "    https://sci-hub.hkvisa.net/10.3102/1076998620934125 page 5-6\n",
        "    \"\"\"\n",
        "    Dz, Dx, n = sorted(Dz), sorted(Dx), len(Dz)\n",
        "    o1 = round(n/2-np.sqrt(n))\n",
        "    o2 = n+1-round(n/2-np.sqrt(n))\n",
        "    alpha_2 = 0.5**n*sum([binom(n,i) for i in range(o1)])\n",
        "    z = norm.ppf(1-alpha_2)\n",
        "\n",
        "    var_z = ((Dz[o2]-Dz[o1])/(2*z))**2\n",
        "    var_x = ((Dx[o2]-Dx[o1])/(2*z))**2\n",
        "    theta = np.median(np.median(Dz)/np.median(Dx))\n",
        "\n",
        "    ln = np.log(theta)\n",
        "    pm = norm.ppf(0.5+conf/2)*np.sqrt(var_z/np.median(Dz)**2+var_x/np.median(Dx)**2)\n",
        "    return np.exp(ln-pm), np.exp(ln+pm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 266,
      "metadata": {
        "id": "4U49X9dl5Uqx"
      },
      "outputs": [],
      "source": [
        "def train_on_particle(model, particle, num_outputs, group, equivariance, epochs, verbose=0):\n",
        "    training_set = create_training_set(particle)\n",
        "    generator = dt.models.lodestar.LodeSTARGenerator(\n",
        "        training_set,\n",
        "        num_outputs=num_outputs, \n",
        "        transformation_function=(group, equivariance), \n",
        "        batch_size=8, \n",
        "        min_data_size=100, \n",
        "        max_data_size=101,\n",
        "    )\n",
        "\n",
        "    const = 10\n",
        "    progress = [] \n",
        "    for _ in tqdm(range(epochs//const)):\n",
        "        with generator:\n",
        "            h = model.fit(generator, epochs=const, steps_per_epoch=100, verbose=verbose)\n",
        "            progress.extend(h.history[\"total_loss\"])\n",
        "            fig, ax = plt.subplots(1,1,figsize=(8,6))\n",
        "            ax.plot(progress)\n",
        "            ax.grid(True)\n",
        "            ax.set_ylim([0, max(progress)+0.5])\n",
        "            ax.set_ylabel(r\"Total loss\", fontsize=16)\n",
        "            ax.set_xlabel(r\"Epok\", fontsize=16)\n",
        "            ax.set_title(\"träning... etc\", fontsize=22)\n",
        "            ax.legend(\"Train\", prop={'size': 15})\n",
        "            plt.xticks(fontsize=12)\n",
        "            plt.yticks(fontsize=12)\n",
        "            plt.savefig(\"training_progress.pdf\")\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "def compare_lode_matlab(model, particles, particles_positions,  verbose=1):\n",
        "    z_mults = []\n",
        "    traces_matlab = []\n",
        "    traces_new = []\n",
        "\n",
        "    for particle_id in tqdm(range(len(particles))):\n",
        "        timeseries = np.stack((np.real(particles[particle_id][:,:,:,0]), np.imag(particles[particle_id][:,:,:,0])), axis=-1)\n",
        "        corrections = predict_on_stack(model, timeseries)\n",
        "\n",
        "        x_lode, y_lode, z_lode = corrections[:,0], corrections[:,1], corrections[:,2]*1.14\n",
        "        x_matlab, y_matlab, z_matlab = particles_positions[particle_id][:,0], particles_positions[particle_id][:,1], particles_positions[particle_id][:,2]*1.14\n",
        "\n",
        "        z_lode = z_lode - np.mean(z_lode)\n",
        "        x_new, y_new, z_new = x_matlab+(x_lode-32)*0.114, y_matlab+(y_lode-32)*0.114, z_matlab+z_lode # byt ut mot pixel_size!\n",
        "\n",
        "        z_mult = minimize(lambda x: np.abs(calc_sigma2(x*z_matlab + z_lode)), [0], method=\"Powell\").x\n",
        "        if verbose:\n",
        "            if particle_id%100==0:\n",
        "                fig, (ax1, ax2) = plt.subplots(1,2,figsize=(16,6))\n",
        "\n",
        "                fig.suptitle('Uppskattade partikelbanor', fontsize=25)\n",
        "                t_range = np.linspace(0, len(z_new)/41, len(z_new)) # OBS: 1/41 i tidssteg i plot\n",
        "                ax1.grid(True)\n",
        "                ax1.set_ylabel(r\"$x$-koordinat [\\SI{}{\\micro\\meter}]\", fontsize=20)\n",
        "                ax1.set_xlabel(r\"Tid [\\SI{}{\\second}]\", fontsize=20)\n",
        "                ax1.plot(t_range, x_matlab, 'b')\n",
        "                ax1.plot(t_range, x_new, 'r')\n",
        "                ax1.legend([\"Klassisk algoritm\", \"LodeSTAR\"], prop={'size': 16})\n",
        "                ax1.tick_params(axis='x', labelsize=16)\n",
        "                ax1.tick_params(axis='y', labelsize=16)\n",
        "\n",
        "                ax2.grid(True)\n",
        "                ax2.set_ylabel(r\"$z$-koordinat [\\SI{}{\\micro\\meter}]\", fontsize=20)\n",
        "                ax2.set_xlabel(r\"Tid [\\SI{}{\\second}]\", fontsize=20)\n",
        "                ax2.plot(t_range, z_matlab, 'b')\n",
        "                ax2.plot(t_range, z_new, 'r')\n",
        "                ax2.legend([\"Klassisk algoritm\", \"LodeSTAR\"], prop={'size': 16})\n",
        "                ax2.tick_params(axis='x', labelsize=16)\n",
        "                ax2.tick_params(axis='y', labelsize=16)\n",
        "                plt.savefig(f\"partikel_{particle_id}.pdf\")\n",
        "                plt.show()\n",
        "\n",
        "        z_mults.append(z_mult)\n",
        "        fac = 1\n",
        "        traces_matlab.append(np.array([x_matlab, y_matlab, z_matlab*fac]))\n",
        "        traces_new.append(np.array([x_matlab, y_matlab, z_new*fac])) #x & y matlab!!\n",
        "    return traces_matlab, traces_new, z_mults"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 267,
      "metadata": {
        "id": "nPZWgA3yv7zM"
      },
      "outputs": [],
      "source": [
        "def plot_diffusion(Dx, Dz, colors, title, xlabel):\n",
        "    range=(-0.5,3*np.median(np.array([Dx, Dz])))\n",
        "    nbins=20\n",
        "\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
        "    ax.hist(Dx, nbins, alpha=0.5, range=range, facecolor=seaborn.color_palette(\"colorblind\")[colors[0]], label=r\"$D_{\\text{x}}$\", edgecolor='grey')\n",
        "    ax.hist(Dz, nbins, alpha=0.5, range=range, facecolor=seaborn.color_palette(\"colorblind\")[colors[1]], label=r\"$D'_{\\text{z}}$\", edgecolor='grey')\n",
        "    ax.set_ylabel(\"Frekvens\", fontsize=20)\n",
        "    ax.set_xlabel(xlabel, fontsize=16)\n",
        "    ax.set_title(title, fontsize=22)\n",
        "    ax.legend(prop={'size': 15})\n",
        "    plt.xticks(fontsize=16)\n",
        "    plt.yticks(fontsize=16)\n",
        "    plt.savefig(\"dist.pdf\", pad_inches=0.2, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "def plot_ratio(Dx, Dz):\n",
        "    Dz, Dx = np.array(Dz), np.array(Dx)\n",
        "    range=(0,3*np.median(Dz/Dx))\n",
        "    nbins=20\n",
        "\n",
        "    lim, _ = confint(Dz, Dx, conf=0.01)\n",
        "    lowlim, highlim = confint(Dz, Dx)\n",
        "    print(f\"Dz/Dx limits: ({lowlim, highlim}), middle: {lim}\")\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "    ax1.hist(Dz/Dx, nbins, alpha=0.5, range=range, facecolor=\"blue\", edgecolor='grey')\n",
        "    ax1.axvspan(lowlim, highlim, alpha=0.3, color='black', label=\"Skattat intervall\", edgecolor='transparent')\n",
        "    #ax1.axvline(1.265625, alpha=1, color='black', linestyle='--', label='Förväntat värde')\n",
        "    ax1.set_ylabel(\"Frekvens\", fontsize=16)\n",
        "    ax1.set_title(r\"Distribution av ratio mellan $D_\\text{z}$ och $D_\\text{x}$\", fontsize=16)\n",
        "    ax1.set_xlabel(r\"$D_\\text{z}/D_\\text{x}$\", fontsize=22)\n",
        "    ax1.legend(prop={'size': 15})\n",
        "\n",
        "    ax2.hist(np.sqrt(np.abs(Dz/Dx)), nbins, alpha=0.5, range=range, facecolor=\"blue\", edgecolor='grey')\n",
        "    ax2.axvspan(np.sqrt(lowlim), np.sqrt(highlim), alpha=0.3, color='black', label=\"Skattat intervall\", edgecolor='transparent')\n",
        "    ax2.axvline(1.125, alpha=1, color='black', linestyle='--', label=\"Förväntat värde\")\n",
        "    ax2.set_ylabel(\"Frekvens\", fontsize=16)\n",
        "    ax2.set_title(r\"Distribution av ratio mellan $n_\\text{z}$ och $n_\\text{x}$\", fontsize=16)\n",
        "    ax2.set_xlabel(r\"$n_\\text{z}/n_\\text{x}$\", fontsize=22)\n",
        "    ax2.legend(prop={'size': 15})\n",
        "    plt.xticks(fontsize=16)\n",
        "    plt.yticks(fontsize=16)\n",
        "    plt.savefig(\"kvot_dist.pdf\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_n(Dx, Dz, n):\n",
        "    Dz, Dx = np.array(Dz), np.array(Dx)\n",
        "\n",
        "    a, b = 0.335, 1.577\n",
        "    n_oil = 1.5\n",
        "    middle = confint(Dz, Dx, conf=0.00001)[0]\n",
        "    n_medium_hat = b/(2*a)-np.sqrt(b**2/(4*a**2)-n_oil*np.sqrt(middle)/a)\n",
        "    print(n_medium_hat)\n",
        "    C_hat = b-a*n_medium_hat\n",
        "    ratio = Dz/Dx\n",
        "    ratio[ratio<0] = 0\n",
        "    ratio = n_oil*np.sqrt(ratio)/C_hat\n",
        "\n",
        "    lowlim, highlim = confint(Dz, Dx)\n",
        "    highlim, lowlim = b/(2*a)-np.sqrt(b**2/(4*a**2)-n_oil*np.sqrt(lowlim)/a), b/(2*a)-np.sqrt(b**2/(4*a**2)-n_oil*np.sqrt(highlim)/a)\n",
        "\n",
        "    nbins=30\n",
        "    range=(0,2.5*np.median(ratio))\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(7, 6))\n",
        "\n",
        "    ax.hist(ratio, nbins, alpha=0.5, range=range, facecolor=\"blue\", edgecolor='grey')\n",
        "    ax.axvspan(lowlim, highlim, alpha=0.4, color='black', label=\"Prediktionsintervall\", edgecolor='transparent')\n",
        "    ax.axvline(n, alpha=1, color='black', linestyle='--', label='Tabellvärde')\n",
        "    ax.set_ylabel(\"Frekvens\", fontsize=20)\n",
        "    dic = {1.33303:\"vatten\", 1.36404:r\"25 procent glycerin\", 1.39809:r\"50 procent glycerin\"}\n",
        "    ax.set_title(r\"Skattat brytningsindex för \"+dic[n], fontsize=24)\n",
        "    ax.set_xlabel(r\"$n_\\text{medium}$\", fontsize=22)\n",
        "    ax.legend(prop={'size': 15})\n",
        "    plt.xticks(fontsize=16)\n",
        "    plt.yticks(fontsize=16)\n",
        "    plt.savefig(f\"brytningsindex{n}.pdf\", pad_inches=0.1, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    return lowlim, highlim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 268,
      "metadata": {
        "id": "pwU83ZnV9xN9"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "def save_weights(path, model):\n",
        "    model.save_weights(path)\n",
        "\n",
        "def load_weights(path, model):\n",
        "    model.load_weights(path)\n",
        "    return model\n",
        "\n",
        "def concatenate_files(path, filenames): #concatenates files pair-wise\n",
        "    files = []\n",
        "    for idx in range(0, len(filenames), 2):\n",
        "        print(f\"Joining {filenames[idx]} with {filenames[idx+1]}\")\n",
        "        first_file = scipy.io.loadmat(path + filenames[idx])['CompletedTraces']\n",
        "        second_file = scipy.io.loadmat(path + filenames[idx+1])['CompletedTraces']\n",
        "        new_file=np.concatenate((first_file, second_file), axis=1)\n",
        "        files.append(new_file)\n",
        "    return files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)    \n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "weights_path = \"/content/drive/My Drive/new_weights/\"\n",
        "data_path = \"/content/drive/My Drive/final_data/\"\n",
        "crop_size = 64\n",
        "padding = 16\n",
        "wavelength = 532-9\n",
        "pixel_size = 114e-9\n",
        "num_outputs = 3\n",
        "image_size = 64\n",
        "delta_time = 1/41\n",
        "num_layers = 7\n",
        "shortest_trace = 80\n",
        "epochs_to_train = 200\n",
        "validation_ids = [1]\n",
        "\n",
        "filenames = !ls \"/content/drive/My Drive/final_data/\"\n",
        "filenames = sorted([filename for filename in filenames if (\".mat\" in filename) and (\"wGold\" in filename) and (\"Silica\" in filename) and (\"0\" in filename)])[:6]\n",
        "print(filenames)\n",
        "\n",
        "try:\n",
        "    model = load_weights(weights_path+\"/\"+str(num_layers), create_model(CustomLayer(), num_layers, image_size, num_outputs))\n",
        "    print(\"WEIGHTS LOADED\")\n",
        "    train_flag = False\n",
        "except:\n",
        "    model = create_model(CustomLayer(), num_layers, image_size, num_outputs)\n",
        "    print(\"WEIGHTS NOT LOADED, NEW MODEL CREATED\")\n",
        "    train_flag = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yghK8erXj8nl",
        "outputId": "20d4a5b1-35cc-4503-b758-2e8ec7a43403"
      },
      "execution_count": 269,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Thu May 12 09:08:45 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P0    33W / 250W |   1597MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "['0wtGlycerol_300nmSilica_wGold_1_3msExp_41fpsEvery1_1_movie_1.aviCompletedTraces.mat', '0wtGlycerol_300nmSilica_wGold_1_3msExp_41fpsEvery1_1_movie_1.aviCompletedTraces0.mat', '0wtGlycerol_300nmSilica_wGold_1_3msExp_41fpsEvery1_1_movie_2.aviCompletedTraces.mat', '0wtGlycerol_300nmSilica_wGold_1_3msExp_41fpsEvery1_1_movie_2.aviCompletedTraces0.mat', '0wtGlycerol_300nmSilica_wGold_1_3msExp_41fpsEvery1_1_movie_3.aviCompletedTraces.mat', '0wtGlycerol_300nmSilica_wGold_1_3msExp_41fpsEvery1_1_movie_3.aviCompletedTraces0.mat']\n",
            "WEIGHTS LOADED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epj5U8pC6rB_",
        "outputId": "99fbe24d-aec6-4b5a-ab1f-13625f102e0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Joined 0wtGlycerol_300nmSilica_wGold_1_3msExp_41fpsEvery1_1_movie_1.aviCompletedTraces.mat with 0wtGlycerol_300nmSilica_wGold_1_3msExp_41fpsEvery1_1_movie_1.aviCompletedTraces0.mat\n",
            "Joined 0wtGlycerol_300nmSilica_wGold_1_3msExp_41fpsEvery1_1_movie_1.aviCompletedTraces.mat with 0wtGlycerol_300nmSilica_wGold_1_3msExp_41fpsEvery1_1_movie_2.aviCompletedTraces.mat\n",
            "Joined 0wtGlycerol_300nmSilica_wGold_1_3msExp_41fpsEvery1_1_movie_1.aviCompletedTraces.mat with 0wtGlycerol_300nmSilica_wGold_1_3msExp_41fpsEvery1_1_movie_2.aviCompletedTraces0.mat\n",
            "Joined 0wtGlycerol_300nmSilica_wGold_1_3msExp_41fpsEvery1_1_movie_1.aviCompletedTraces.mat with 0wtGlycerol_300nmSilica_wGold_1_3msExp_41fpsEvery1_1_movie_3.aviCompletedTraces.mat\n",
            "Joined 0wtGlycerol_300nmSilica_wGold_1_3msExp_41fpsEvery1_1_movie_1.aviCompletedTraces.mat with 0wtGlycerol_300nmSilica_wGold_1_3msExp_41fpsEvery1_1_movie_3.aviCompletedTraces0.mat\n",
            "Loading file: 0wtGlycerol_300nmSilica_wGold_1_3msExp_41fpsEvery1_1_movie_3.aviCompletedTraces0.mat "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 99%|█████████▉| 4626/4657 [00:45<00:00, 91.43it/s]"
          ]
        }
      ],
      "source": [
        "sigma_2_lode, sigma_2_matlab = [], []\n",
        "ns = {\"0\":1.33303, \"2\":1.36404, \"5\":1.39809} #http://edge.rit.edu/edge/P13051/public/Research%20Notes/refractive%20index%20glycerin%20water.pdf\n",
        "current_file_index = 0\n",
        "while current_file_index < len(filenames):\n",
        "    file = scipy.io.loadmat(data_path + filenames[current_file_index])['CompletedTraces']\n",
        "\n",
        "    i=1\n",
        "    while current_file_index+i<len(filenames) and filenames[current_file_index][:50] in filenames[current_file_index+i]:\n",
        "        second_file = scipy.io.loadmat(data_path + filenames[current_file_index+i])['CompletedTraces']\n",
        "        file=np.concatenate((file, second_file), axis=1)\n",
        "        print(f\"Joined {filenames[current_file_index]} with {filenames[current_file_index+i]}\")\n",
        "        i+=1\n",
        "    current_file_index +=i\n",
        "\n",
        "    #file = file[:,:300]\n",
        "    print(f\"Loading file: {filenames[current_file_index-1]}\", end=\" \")\n",
        "    particles_positions, particles_timeseries = extract_pos_and_images(file, shortest_trace=shortest_trace)\n",
        "\n",
        "    group, equivariance, prop_matrices = create_group(crop_size=crop_size, padding=padding, wavelength=wavelength, pixel_size=pixel_size)\n",
        "    #plot_dataset(create_training_set(particles_timeseries), group, nrows=1)\n",
        "\n",
        "    if train_flag:\n",
        "        train_on_particle(model, particles_timeseries, num_outputs, group, equivariance, epochs_to_train, verbose=0) # change verbose to show training progress or not.\n",
        "        #save_weights(weights_path+\"/\"+str(num_layers), model)\n",
        "        train_flag = False\n",
        "    else:\n",
        "        train_flag = False\n",
        "\n",
        "    #for validation_id in validation_ids:\n",
        "    #    plot_consistency(model, particles_timeseries[validation_id], np.linspace(-80,80,50), prop_matrices, padding)\n",
        "\n",
        "    print(f\"Creating corrected traces\", end=\" \")\n",
        "    traces_matlab, traces_lode, z_mults = compare_lode_matlab(model, particles_timeseries, particles_positions, verbose=0) # change verbose to plot paths or not.\n",
        "    print(f\"OPTIMAL Z_MULT: {np.median(z_mults)}\")\n",
        "\n",
        "    Dx, Dz, sigma_z2 = calculate_diffusion(traces_lode, delta_time, shortest_trace=shortest_trace)\n",
        "    plot_diffusion(Dx, Dz, (9,3), \"Diffusionskonstanter parallellt respektive normalt mot kameraplanet\", r\"$D_\\text{x}$ och $D'_\\text{z}$ [\\SI{}{\\micro\\meter^2\\second^{-1}}]\")\n",
        "    #plot_ratio(Dx, Dz)\n",
        "    low, high = plot_n(Dx, Dz, ns[filenames[current_file_index-1][0]])\n",
        "    print(low, high)\n",
        "    #print(f\"intervall: ({low:.3f}, {high:.3f}), teori: {ns[filenames[current_file_index-1][0]]:.3f}\")\n",
        "    sigma_2_lode.append(sigma_z2)\n",
        "\n",
        "    #Dx, Dz, sigma_z2 = calculate_diffusion(traces_matlab, delta_time, shortest_trace=shortest_trace)\n",
        "    #plot_diffusion(Dx, Dz, (9,3), \"Diffusionskonstanter tangentiellt och parallellt med kameraplanet\", r\"$D_\\text{x}$ och $D_\\text{z}$ [\\SI{}{\\micro\\meter^2\\second^{-1}}]\")\n",
        "    #plot_ratio(Dx, Dz)\n",
        "    #low, high = ppltlot_n(Dx, Dz, ns[filenames[current_file_index-1][0]])\n",
        "    #print(f\"intervall: ({low:.3f}, {high:.3f}), teori: {ns[filenames[current_file_index-1][0]]:.3f}\")\n",
        "    #sigma_2_matlab.append(sigma_z2)\n",
        "\n",
        "sigma_2_lode = np.array(sigma_2_lode)\n",
        "sigma_2_matlab = np.array(sigma_2_matlab)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_diffusion(Dx, Dz, (9,3), \"Diffusionskonstanter parallellt respektive normalt mot kameraplanet\", r\"$D_\\text{x}$ och $D_\\text{z}$ [\\SI{}{\\micro\\meter^2\\second^{-1}}]\")"
      ],
      "metadata": {
        "id": "FLSDRI_zPjrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from matplotlib.transforms import Affine2D\n",
        "y_lode, yerr_lode, y_matlab, yerr_matlab = [], [], [], []\n",
        "for i in range(3):\n",
        "    lode = sorted(sigma_2_lode[i])\n",
        "    matlab = sorted(sigma_2_matlab[i])\n",
        "    \n",
        "    n = len(lode)\n",
        "    o1 = round(n/2-np.sqrt(n))\n",
        "    o2 = n+1-round(n/2-np.sqrt(n))\n",
        "    alpha_2 = 0.5**n*sum([binom(n,i) for i in range(o1)])\n",
        "    z = norm.ppf(1-alpha_2)\n",
        "    var_lode = ((lode[o2]-lode[o1])/(2*z))**2\n",
        "    var_matlab = ((lode[o2]-lode[o1])/(2*z))**2\n",
        "\n",
        "    y_lode.append(np.sqrt(np.median(lode)))\n",
        "    y_matlab.append(np.sqrt(np.median(matlab)))\n",
        "    yerr_lode.append(var_lode+0.003)\n",
        "    yerr_matlab.append(var_matlab+0.003)\n",
        "\n",
        "lode_data = {\n",
        "    'x': list(range(3)),\n",
        "    'y': y_lode,\n",
        "    'yerr': yerr_lode} # intervallgränser undre och övre\n",
        "matlab_data = {\n",
        "    'x': list(range(3)),\n",
        "    'y': y_matlab,\n",
        "    'yerr': yerr_matlab} # intervallgränser undre och övre\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(1,1, figsize=(8,6))\n",
        "for data in [matlab_data, lode_data]:\n",
        "    #ax.errorbar(**data, alpha=.8, fmt='.', capsize=7, capthick=2, markersize=0)\n",
        "    ax.fill_between(data['x'], data['y']-np.array(data['yerr'])/2, data['y']+np.array(data['yerr'])/2, alpha=.8)\n",
        "    ax.set_title(\"Positionsosäkerhet för olika dataset\", fontsize=22)\n",
        "    ax.set_ylabel(r\"Genomsnittlig positionsosäkerhet $\\sigma_z$\", fontsize=20)\n",
        "    ax.grid(True)\n",
        "    ax.set_ylim((0,0.3))\n",
        "\n",
        "    my_xticks = [r\"\\SI{0}{\\percent} glycerin\", r\"\\SI{25}{\\percent} glycerin\", r\"\\SI{50}{\\percent} glycerin\"]\n",
        "    plt.xticks(list(range(3)), my_xticks, fontsize=16)\n",
        "\n",
        "ax.legend([\"Klassisk algoritm\", \"LodeSTAR\"], prop={'size': 15})\n",
        "#ax.plot(actual_data['x'], actual_data['y'], '-.k', alpha=0.7)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.savefig(\"3_datasets.pdf\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yGYJA5sDCLhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot([0,1], [0,2])\n",
        "plt.xticks(fontsize=16)"
      ],
      "metadata": {
        "id": "6iEBiC3tb-Ji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = scipy.io.loadmat(data_path + \"0wtGlycerol_300nmSilica_wGold_1_3msExp_41fpsEvery1_1_movie_1.aviCompletedTraces0\")['CompletedTraces'][:,:100]\n",
        "particles_positions, particles_timeseries = extract_pos_and_images(file, shortest_trace=shortest_trace)\n",
        "\n",
        "snrs, sigmas = [], []\n",
        "ALL_USED_DATA = []\n",
        "for id in range(3):\n",
        "  print(id)\n",
        "  chosen_trace = particles_timeseries[0]\n",
        "\n",
        "  x = np.arange(image_size) - image_size/2\n",
        "  y = np.arange(image_size) - image_size/2\n",
        "  X, Y = np.meshgrid(x,y)\n",
        "  RHO = np.sqrt(X**2 + Y**2)\n",
        "  steps=9\n",
        "  noisy_sets = []\n",
        "  for i in range(steps):\n",
        "    noisy_set=[]\n",
        "    for img in chosen_trace:\n",
        "      fft_img = np.fft.fftshift(np.fft.fft2(img[..., 0]))\n",
        "      tmp = (1*np.random.randn(64, 64) + 1j * np.random.randn(64, 64))*i*1.5\n",
        "      tmp[RHO>=11]= 0\n",
        "      noisy_set.append(np.fft.ifft2(np.fft.ifftshift(fft_img+tmp)))\n",
        "    noisy_sets.append(noisy_set)\n",
        "\n",
        "  noisy_sets = np.array(noisy_sets)\n",
        "  noisy_sets = np.moveaxis(noisy_sets, [0,1,2,3], [3,0,1,2])\n",
        "  ALL_USED_DATA.append(noisy_sets)\n",
        "  \n",
        "ALL_USED_DATA = np.array(ALL_USED_DATA)"
      ],
      "metadata": {
        "id": "9yjd9u-8lnPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#np.save(\"/content/drive/My Drive/SNR/snr_data.npy\", ALL_USED_DATA)\n",
        "print(ALL_USED_DATA.shape)\n",
        "#AXIS 0: ITERATION, AXIS 1: BILDNUMMER I TRACE, AXIS 2&3: 64x64 FÄLTBILD, AXIS 4: BRUSNIVÅ, INDEX 0 LÄGST"
      ],
      "metadata": {
        "id": "ciTuPdsWvQC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(1,3,figsize=(18,6))\n",
        "#!pip install matplotlib-scalebar\n",
        "from matplotlib_scalebar.scalebar import ScaleBar\n",
        "for i in range(3):\n",
        "  for j in range(3):\n",
        "    idx = 3*i+j\n",
        "    current_set = ALL_USED_DATA[0,:,:,:,idx:idx+1]\n",
        "\n",
        "    mean_image = np.array([np.mean(current_set, axis=0) for i in range(current_set.shape[0])])\n",
        "    signal = np.sum(np.abs(mean_image[0,:,:,0]))\n",
        "    noise_and_signal = np.sum(np.abs(current_set[0,:,:,0]))\n",
        "    noise = noise_and_signal - signal\n",
        "    if i==j:\n",
        "        snr = signal/noise\n",
        "        axs[i].imshow(np.abs(ALL_USED_DATA[0,0,:,:,idx]), cmap='bone')\n",
        "        #axs[i].add_artist(ScaleBar(0.01, units=\"um\", length_fraction=1))\n",
        "        axs[i].set_xticks([])#np.linspace(0, 63, 5))\n",
        "        #axs[i].set_xticklabels([])#([f\"{fl:.2f}\" for fl in np.linspace(0, 8.96, 5)])\n",
        "        axs[i].set_yticks([])#np.linspace(0, 63, 5))\n",
        "        #axs[i].set_yticklabels([])#([f\"{fl:.2f}\" for fl in np.linspace(0, 8.96, 5)])\n",
        "\n",
        "        fig.suptitle(\"Intensitetsbilder med varierande SNR\", fontsize=22)\n",
        "        axs[i].set_xlabel(f\"SNR: {snr:.2f}\", fontsize=16)\n",
        "\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.savefig(\"added_noise.pdf\", pad_inches=0.05, bbox_inches='tight')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_koHtbrtmLH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.transforms import Affine2D\n",
        "\n",
        "print(sigma_2_lode.shape)\n",
        "y_lode = [(np.sqrt(np.median(l))) for l in sigma_2_lode]\n",
        "yerr_lode = [np.sqrt(np.std(l)) for l in sigma_2_lode]\n",
        "y_matlab = [(np.sqrt(np.median(l))) for l in sigma_2_matlab]\n",
        "yerr_matlab = [np.sqrt(np.std(l)) for l in sigma_2_matlab]\n",
        "\n",
        "actual_data = {\n",
        "    'x': list(range(3)),\n",
        "    'y': [1.33303, 1.36404, 1.39809],}\n",
        "lode_data = {\n",
        "    'x': list(range(3)),\n",
        "    'y': y_lode,\n",
        "    'yerr': yerr_lode} # intervallgränser undre och övre\n",
        "matlab_data = {\n",
        "    'x': list(range(3)),\n",
        "    'y': y_matlab,\n",
        "    'yerr': yerr_matlab} # intervallgränser undre och övre\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(1,1, figsize=(8,6))\n",
        "ax.plot(actual_data['x'], actual_data['y'], 'sk', alpha=1, markersize=8)\n",
        "for data in [matlab_data, lode_data]:\n",
        "    ax.errorbar(**data, alpha=.8, fmt='.', capsize=7, capthick=2, markersize=0)\n",
        "    ax.set_title(\"Uppskattade brytningsindex\", fontsize=22)\n",
        "    ax.set_ylabel(r\"Predikterat brytningsindex {n}\", fontsize=16)\n",
        "    ax.grid(True)\n",
        "\n",
        "    my_xticks = [r\"\\SI{0}{\\percent} glycerin\", r\"\\SI{25}{\\percent} glycerin\", r\"\\SI{50}{\\percent} glycerin\"]\n",
        "    plt.xticks(list(range(3)), my_xticks)\n",
        "    \n",
        "ax.legend([\"Känt brytningsindex\", \"Klassisk algoritm\", \"LodeSTAR\"], prop={'size': 15})\n",
        "#ax.plot(actual_data['x'], actual_data['y'], '-.k', alpha=0.7)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rb0XEHaWz56_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#noise_data = np.load(\"/content/drive/My Drive/SNR/snr_data.npy\")\n",
        "\"IMPORTERA MATLAB-Z\"\n",
        "\"TA PARTIKELPOS-Z\"\n",
        "\"ADDERA\"\n",
        "\"z_corrections_matlab_snr = ...\"\n",
        "noise_data = ALL_USED_DATA\n",
        "print(noise_data.shape)\n",
        "sigmas_lode, sigmas_matlab = [], []\n",
        "snrs = []\n",
        "for i in range(noise_data.shape[0]):\n",
        "  noisy_sets = noise_data[i,...]\n",
        "  particle_id = 0\n",
        "  snr = []\n",
        "  sigma_lode, sigma_matlab = [], []  \n",
        "  for step in range(noisy_sets.shape[-1]):\n",
        "      print(step)\n",
        "      current_set = noisy_sets[:,:,:,step:step+1]\n",
        "      mean_image = np.array([np.mean(current_set, axis=0) for i in range(current_set.shape[0])])\n",
        "\n",
        "      signal = np.sum(np.abs(mean_image[0,:,:,0]))\n",
        "      noise_and_signal = np.sum(np.abs(current_set[0,:,:,0]))\n",
        "      noise = noise_and_signal - signal\n",
        "\n",
        "      traces_matlab, traces_lode, z_mults = compare_lode_matlab(model, [current_set], [particles_positions[particle_id]], verbose=0)\n",
        "      sigma2_lode = calc_sigma2(traces_lode[0][2])\n",
        "      #sigma2_matlab = calc_sigma2(traces_matlab[0][2]+z_corrections_matlab_snr)\n",
        "\n",
        "      snr.append(signal/noise)\n",
        "      sigma_lode.append(np.sqrt(sigma2_lode))\n",
        "      #sigma_matlab.append(np.sqrt(sigma2_lode))\n",
        "\n",
        "  snrs.append(snr)\n",
        "  sigmas_lode.append(sigma_lode)\n",
        "  #sigmas_matlab.append(sigma_matlab)\n",
        "\n",
        "  y_lode = np.mean(np.array(sigmas_lode), axis=0)\n",
        "  yerr_lode = np.std(np.array(sigmas_lode), axis=0)\n",
        "  #y_matlab = np.mean(np.array(sigmas_matlab), axis=0)\n",
        "  #yerr_matlab = np.std(np.array(sigmas_matlab), axis=0)\n",
        "  x = np.median(np.array(snrs), axis=0)\n",
        "\n",
        "  fig, ax = plt.subplots(1,1,figsize=(8,6))\n",
        "  ax.errorbar(x, y_lode, yerr=yerr_lode*2, alpha=.8, fmt='.', capsize=7, capthick=2, markersize=0)\n",
        "  #ax.errorbar(x, y_matlab, yerr=yerr_matlab*2, alpha=.8, fmt='.', capsize=7, capthick=2, markersize=0)\n",
        "  ax.plot(snr, sigma_lode, 'k-.', alpha=0.2)\n",
        "  ax.grid(True)\n",
        "  ax.set_xlabel(\"SNR\")\n",
        "  ax.set_ylabel(\"Sigma\")\n",
        "  ax.legend([\"LodeSTAR\", \"Klassisk algoritm\"], prop={'size': 15})\n",
        "  #ax.set_ylim([0,3])\n",
        "  #ax.set_xlim([0,3])\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "2QjTGnVpobJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = scipy.io.loadmat(data_path + filenames[0])['CompletedTraces']\n",
        "particles_positions, particles_timeseries = extract_pos_and_images(file, shortest_trace=shortest_trace)\n",
        "models=[]\n",
        "epochs_to_train=60\n",
        "for _ in range(20):\n",
        "    models.append(load_weights(weights_path+\"/\"+str(num_layers), create_model(CustomLayer(), num_layers, image_size, num_outputs)))\n",
        "\n",
        "for i, model in enumerate(models):\n",
        "    train_on_particle(model, particles_timeseries, num_outputs, group, equivariance, epochs_to_train, verbose=0)\n",
        "    save_weights(weights_path+\"/\"+str(num_layers)+\"/\"+str(i), model)\n",
        "    traces_matlab, traces_lode, z_mults = compare_lode_matlab(model, particles_timeseries, particles_positions, verbose=0)\n",
        "    Dx, Dz = calculate_diffusion(traces_lode, delta_time, shortest_trace=shortest_trace)\n",
        "    low, high = plot_n(Dx, Dz, ns[filenames[0][0]])\n",
        "    print(f\"intervall: ({low:.3f}, {high:.3f}), teori: {ns[filenames[current_file_index-1][0]]:.3f}\")\n",
        "    n_range_lode.append((low, high))\n"
      ],
      "metadata": {
        "id": "Z7X-pztYcyuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y_lode, yerr_lode = np.mean(n_range_lode, axis=1), np.squeeze(np.diff(n_range_lode, axis=1))\n",
        "\n",
        "actual_data = {\n",
        "    'x': list(range(3)),\n",
        "    'y': 1.33303*np.ones(len(y_lode)),}\n",
        "lode_data = {\n",
        "    'x': list(range(3)),\n",
        "    'y': y_lode,\n",
        "    'yerr': yerr_lode} # intervallgränser undre och övre\n",
        "\n",
        "fig, ax = plt.subplots(1,1, figsize=(8,6))\n",
        "ax.plot(actual_data['x'], actual_data['y'], 'sk', alpha=1, markersize=8)\n",
        "for data in [lode_data]:\n",
        "    ax.errorbar(**data, alpha=.8, fmt='.', capsize=7, capthick=2, markersize=0)\n",
        "    ax.set_title(\"Uppskattade brytningsindex\", fontsize=22)\n",
        "    ax.set_ylabel(r\"Predikterat brytningsindex {n}\", fontsize=16)\n",
        "    ax.grid(True)"
      ],
      "metadata": {
        "id": "ECQOW7u4kbPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1,1)\n",
        "ax.plot([0, 0.1], [0, 0.2])\n",
        "ax.yaxis.set_major_formatter(y_format)\n",
        "ax.xaxis.set_major_formatter(y_format)"
      ],
      "metadata": {
        "id": "tJ76yMYY4aat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://omlc.org/cgi-bin/tmp_data.pl?name=%2Ftmp%2F165199918261788_data.txt\n",
        "\n",
        "\n",
        "#angledata = np.genfromtxt('240nmPSL', delimiter=',')\n",
        "angledata = np.genfromtxt('angledata.txt', delimiter=',')\n",
        "thetas=np.radians(angledata[:,0])\n",
        "perpen=angledata[:,2]\n",
        "\n",
        "NA = 1.3\n",
        "n = 1.33303\n",
        "noil = 1.50\n",
        "integral_lim = np.arcsin(NA/noil)\n",
        "fig, ax = plt.subplots(1,1)\n",
        "x = np.linspace(1.33303, 1.4, 10)#[1.33303, 1.36404, 1.39809]\n",
        "y = []\n",
        "for n in x:\n",
        "    integral = 0\n",
        "    norm = 0\n",
        "    for theta, I in zip(thetas, perpen):\n",
        "        if 0 < theta < integral_lim:\n",
        "            integral += np.cos(theta)*np.sin(theta)*I*np.sqrt(1-n**2/noil**2*np.sin(theta)**2)/np.cos(theta)\n",
        "            norm += np.cos(theta)*np.sin(theta)*I\n",
        "    y.append(integral/norm+0.070967)\n",
        "\n",
        "def f(n_medium):\n",
        "    return 1.577-0.335*n_medium\n",
        "\n",
        "plt.plot(x, f(x), '-.')\n",
        "ax.plot(x, y, 'kX')\n",
        "\n",
        "ax.set_xlabel(r\"mediets brytningsindex $n_\\text{medium}$\", fontsize=19)\n",
        "ax.set_ylabel(r\"C\", fontsize=19)\n",
        "ax.grid(True)\n",
        "    #print(f\"factor: {integral/norm}\")\n",
        "plt.title(\"\")\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.title(r\"Värdet $C$ mot $n_\\text{medium}$\", fontsize=25)\n",
        "plt.savefig(\"c_plot.pdf\", pad_inches=0.1, bbox_inches='tight')\n",
        "\n",
        "\n",
        "#ADDERA 0.070967 till C"
      ],
      "metadata": {
        "id": "inlZNwYF1iP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = np.genfromtxt('4xlambda')\n",
        "data2 = np.genfromtxt('1xlambda')\n",
        "data3 = np.genfromtxt('025xlambda')\n",
        "\n",
        "thetas = np.radians(data1[:,0])\n",
        "normal1 = data1[:,1]\n",
        "normal2 = data2[:,1]\n",
        "normal3 = data3[:,1]\n",
        "\n",
        "fig,axs = plt.subplots(1,3,figsize=(24,6))\n",
        "axs[0] = plt.subplot(131, projection='polar')\n",
        "axs[1] = plt.subplot(132, projection='polar')\n",
        "axs[2] = plt.subplot(133, projection='polar')\n",
        "plt.suptitle(r\"Miespridning för sfärer med olika diameter $d$ i förhållande till ljusvåglängd $\\lambda$\", fontsize=40)\n",
        "axs[0].set_xlabel(r\"$d = 4 \\lambda$\", fontsize=20)\n",
        "axs[1].set_xlabel(r\"$d = 1 \\lambda$\", fontsize=20)\n",
        "axs[2].set_xlabel(r\"$d = 0.25 \\lambda$\", fontsize=20)\n",
        "axs[0].fill_between(thetas, normal1, linewidth=2, alpha=0.6)\n",
        "axs[1].fill_between(thetas, normal2, linewidth=2, alpha=0.6)\n",
        "axs[2].fill_between(thetas, normal3, linewidth=2, alpha=0.6)\n",
        "axs[0].set_ylim([0,20])\n",
        "axs[1].set_ylim([0,1.6])\n",
        "axs[2].set_ylim([0,0.2])\n",
        "\n",
        "for i in range(3):\n",
        "    axs[i].set_yticks([])\n",
        "    axs[i].set_xticks([])\n",
        "    axs[i].plot([np.pi, np.pi], [0,100], linewidth=2, alpha=1)\n",
        "    axs[i].scatter(0, 0, marker='o', color='black')\n",
        "    axs[i].spines['polar'].set_visible(False)\n",
        "\n",
        "axs[0].set_xlabel(r\"$d = 4 \\lambda$\", fontsize=30)\n",
        "axs[1].set_xlabel(r\"$d = \\lambda$\", fontsize=30)\n",
        "axs[2].set_xlabel(r\"$d = 0.25 \\lambda$\", fontsize=30)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.savefig(\"mie-scatter.pdf\", pad_inches=0.05, bbox_inches='tight')"
      ],
      "metadata": {
        "id": "0sPIgwrNtavi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = 100\n",
        "x = np.pi/3\n",
        "\n",
        "theta = np.linspace(-180,180,180)\n",
        "mu = np.cos(theta/180*np.pi)\n",
        "scat = miepython.i_unpolarized(m,x,mu)\n",
        "\n",
        "fig,ax = plt.subplots(1,2,figsize=(12,5))\n",
        "ax=plt.subplot(121, projection='polar')\n",
        "ax.plot(theta/180*np.pi,scat)\n",
        "ax.set_rticks([0.05, 0.1,0.15]) \n",
        "ax.set_title(\"m=1.5, Sphere Diameter = $\\lambda$/3\")\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.plot(theta,scat)\n",
        "plt.xlabel('Exit Angle [degrees]')\n",
        "plt.ylabel('Unpolarized Scattered light [1/sr]')\n",
        "plt.title('m=1.5, Sphere Diameter = $\\lambda$/3')\n",
        "plt.ylim(0.00,0.2)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GvebtsXato1v"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "TeX_LodeSTAR_20220512.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}